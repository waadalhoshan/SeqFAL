# -*- coding: utf-8 -*-
"""V2 SeqFAL.ipynb

Automatically generated by Colab.

#Imports & Datasets
"""

!pip install -q sentence-transformers scikit-learn numpy pandas openpyxl

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import LinearSVC
from sklearn.calibration import CalibratedClassifierCV
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import shuffle
import random
import itertools

from sentence_transformers import SentenceTransformer

# ===== Enforce GPU in Colab =====
import torch

if not torch.cuda.is_available():
    raise RuntimeError(
        "GPU not available. "
        "In Colab: Runtime → Change runtime type → Hardware accelerator → GPU."
    )

device = torch.device("cuda")
print("Using device:", device)
print("GPU:", torch.cuda.get_device_name(0))

url = 'https://docs.google.com/spreadsheets/d/1_fT878ebTuuGj1f09TYEqB8Fe0XLLCgH/edit?usp=drive_link&ouid=112068552438632279841&rtpof=true&sd=true'
url_promise = "https://docs.google.com/spreadsheets/d/1rP7sIc6VuylVpBsn09skdlOskoDlzotp/edit?usp=drive_link&ouid=112068552438632279841&rtpof=true&sd=true"

#General methods for datasets preps
import pandas as pd
def read_dataset_from_google(url):
  url='https://drive.google.com/uc?id=' + url.split('/')[-2]
  df = pd.read_excel(url)
  return df

df = read_dataset_from_google(url)
df_promise = read_dataset_from_google(url_promise)

df

df_promise["Label"] = df_promise["Label"].apply(
    lambda x: "Security" if x.lower() == "security" else "Non-Security"
)

df_promise

df_promise['ProjectID'].unique()

data = pd.concat([df, df_promise], ignore_index=True)

requirements = data['RequirementText'].tolist()
labels = data['Label'].tolist()

# Encode labels to 0/1
le = LabelEncoder()
y = le.fit_transform(labels)

print("Label mapping:", dict(zip(le.classes_, le.transform(le.classes_))))
print("y:", y)

"""# Create Embeddings"""

model = SentenceTransformer('all-MiniLM-L6-v2')
X = np.array(model.encode(requirements))
#thearod5/bert4re

# ===== Reproducible RNG helper =====
def make_rng(seed: int):
    return np.random.default_rng(seed)

"""#SeqFAL

### Helper: Dirichlet non-IID partition
"""

# ===== Cell 6: Dirichlet label-skew partitioning (non-IID) =====
def dirichlet_label_skew_partition(y_train, K, alpha, seed):
    """
    Returns: list of arrays, each array are indices (into y_train) for that client.
    Non-IID label skew via Dirichlet over class proportions.
    """
    rng = make_rng(seed)
    y_train = np.asarray(y_train)
    n = len(y_train)

    classes = np.unique(y_train)
    class_indices = {c: np.where(y_train == c)[0] for c in classes}
    for c in classes:
        rng.shuffle(class_indices[c])

    # Dirichlet proportions per class across clients
    client_indices = [[] for _ in range(K)]

    for c in classes:
        idx_c = class_indices[c]
        if len(idx_c) == 0:
            continue

        proportions = rng.dirichlet(alpha * np.ones(K))
        # turn proportions into counts
        counts = (proportions * len(idx_c)).astype(int)

        # fix rounding mismatch
        diff = len(idx_c) - np.sum(counts)
        if diff > 0:
            # distribute leftover
            for i in rng.choice(np.arange(K), size=diff, replace=True):
                counts[i] += 1
        elif diff < 0:
            # remove extras (rare)
            for i in rng.choice(np.where(counts > 0)[0], size=-diff, replace=True):
                counts[i] -= 1

        start = 0
        for k in range(K):
            take = counts[k]
            if take > 0:
                client_indices[k].extend(idx_c[start:start+take].tolist())
                start += take

    # shuffle client pools
    out = []
    for k in range(K):
        idx = np.array(client_indices[k], dtype=int)
        rng.shuffle(idx)
        out.append(idx)

    return out

# ===== Cell 7 (UPDATED): Per-client distribution tracking + round logging =====
def summarize_client_pools(y_train, clients_indices, seed, K, alpha):
    rows = []
    for cid, idx in enumerate(clients_indices):
        y_local = y_train[idx]
        n0 = int(np.sum(y_local == 0))
        n1 = int(np.sum(y_local == 1))
        rows.append({
            "seed": seed,
            "K": K,
            "alpha": alpha,
            "client_id": cid,
            "pool_size": len(idx),
            "pool_n0": n0,
            "pool_n1": n1,
            "pool_p0": n0 / max(1, len(idx)),
            "pool_p1": n1 / max(1, len(idx)),
        })
    return pd.DataFrame(rows)

def log_round_state(y_train, seed, K, alpha, round_id, client_id, L_idx, U_idx, forced_random_sampling):
    """
    Logs per-client labeled/unlabeled class distributions per round,
    plus whether we had to force random sampling due to single-class L.
    """
    yL = y_train[L_idx]
    yU = y_train[U_idx]
    return {
        "seed": seed,
        "K": K,
        "alpha": alpha,
        "round": round_id,
        "client_id": client_id,

        "L_size": len(L_idx),
        "L_n0": int(np.sum(yL == 0)),
        "L_n1": int(np.sum(yL == 1)),

        "U_size": len(U_idx),
        "U_n0": int(np.sum(yU == 0)),
        "U_n1": int(np.sum(yU == 1)),

        "forced_random_sampling": int(forced_random_sampling),
    }

# ===== Cell X (NEW): One-class client handling helpers =====
def has_two_classes(y, idx):
    idx = np.asarray(idx, dtype=int)  # <-- force int
    if idx.size == 0:
        return False
    return np.unique(y[idx]).size >= 2

def enforce_two_classes(y, L_idx, U_idx, rng, max_tries=5):
    L = list(np.asarray(L_idx, dtype=int))
    U = list(np.asarray(U_idx, dtype=int))

    tries = 0
    while np.unique(y[np.asarray(L, dtype=int)]).size < 2 and len(U) > 0 and tries < max_tries:
        pick = int(rng.choice(U))
        U.remove(pick)
        L.append(pick)
        tries += 1

    return np.asarray(L, dtype=int), np.asarray(U, dtype=int)

"""### Helper: Federated averaging (linear models)"""

# ===== Cell 10: FedAvg for linear models (including calibrated wrappers) =====
def _extract_linear_params(clf):
    """
    Returns (w, b) for a binary linear model.
    Works for LogisticRegression, LinearSVC, SGDClassifier,
    and CalibratedClassifierCV wrapping a linear estimator.
    """
    # If calibrated, use base_estimator fitted inside calibration.
    # CalibratedClassifierCV stores fitted calibrators; each fold has an estimator.
    # We'll average the underlying linear estimators in the calibrator ensemble.
    if isinstance(clf, CalibratedClassifierCV):
        # Each element has .estimator (the trained base model) + calibrator
        estimators = [cc.estimator for cc in clf.calibrated_classifiers_]
        ws, bs = [], []
        for est in estimators:
            ws.append(est.coef_.reshape(-1))
            bs.append(float(est.intercept_.reshape(-1)[0]))
        w = np.mean(np.vstack(ws), axis=0)
        b = float(np.mean(bs))
        return w, b

    # Non-calibrated linear estimators
    w = clf.coef_.reshape(-1)
    b = float(clf.intercept_.reshape(-1)[0])
    return w, b

def fedavg_linear(local_params, weights):
    """
    local_params: list of (w, b)
    weights: list of floats (e.g., labeled set sizes)
    """
    W = np.array(weights, dtype=float)
    W = W / np.sum(W)

    ws = np.vstack([p[0] for p in local_params])
    bs = np.array([p[1] for p in local_params], dtype=float)

    w_global = np.sum(ws * W[:, None], axis=0)
    b_global = float(np.sum(bs * W))
    return w_global, b_global

def linear_predict_from_params(X, w, b):
    scores = X @ w + b
    return (scores >= 0).astype(int)

"""### Classifier helpers (linear & compatible)"""

#from sklearn.svm import LinearSVC
#from sklearn.linear_model import SGDClassifier, LogisticRegression
#from sklearn.calibration import CalibratedClassifierCV

CALIB_CV = 2  # keep 2 for speed

def make_classifier(name: str, seed: int, need_proba: bool):
    name = name.lower()

    # Native proba
    if name == "logreg":
        return SGDClassifier(
            loss="log_loss",
            penalty="l2",
            alpha=1e-4,
            learning_rate="optimal",
            max_iter=1000,
            tol=1e-3,
            random_state=seed
        )

    # Base linear SVM-style models (fast, no proba)
    if name == "linearsvc":
        base = LinearSVC(random_state=seed)
        return CalibratedClassifierCV(base, method="sigmoid", cv=CALIB_CV) if need_proba else base

    if name == "sgd_hinge":
        base = SGDClassifier(loss="hinge", random_state=seed)
        return CalibratedClassifierCV(base, method="sigmoid", cv=CALIB_CV) if need_proba else base

    raise ValueError(f"Unknown classifier: {name}")

"""### Query strategy helpers"""

# ===== Cell 8: Query strategies (random seeded, LC, margin) =====
def query_by_random(U_idx, k, rng):
    k = min(k, len(U_idx))
    if k <= 0:
        return np.array([], dtype=int)
    return rng.choice(U_idx, size=k, replace=False)

def query_by_least_confidence(clf, X, U_idx, k):
    """
    Least Confidence: choose smallest max class probability.
    """
    k = min(k, len(U_idx))
    if k <= 0:
        return np.array([], dtype=int)

    proba = clf.predict_proba(X[U_idx])
    maxp = np.max(proba, axis=1)
    order = np.argsort(maxp)  # ascending => least confident first
    return U_idx[order[:k]]

def query_by_margin(clf, X, U_idx, k):
    """
    SeqFAL proposed margin: select points closest to decision boundary.
    For linear SVM-style models, use |decision_function(x)|.
    Smaller => closer => more informative.
    """
    k = min(k, len(U_idx))
    if k <= 0:
        return np.array([], dtype=int)

    scores = clf.decision_function(X[U_idx])
    margins = np.abs(scores)
    order = np.argsort(margins)  # ascending => smallest margin first
    return U_idx[order[:k]]

# ===== Cell X: One-class client handling =====
def has_two_classes(y, idx):
    idx = np.asarray(idx, dtype=int)
    if idx.size == 0:
        return False
    return np.unique(y[idx]).size >= 2

def enforce_two_classes(y, L_idx, U_idx, rng, max_tries=5):
    """
    Randomly samples from U until L has at least two classes,
    or U is exhausted.
    """
    L = L_idx.tolist()
    U = U_idx.tolist()

    tries = 0
    while not has_two_classes(y, np.array(L)) and len(U) > 0 and tries < max_tries:
        pick = rng.choice(U)
        U.remove(pick)
        L.append(pick)
        tries += 1

    return np.array(L, dtype=int), np.array(U, dtype=int)

"""## Experiments Skelton"""

def run_seqfal(
    X_train, y_train, X_test, y_test,
    K, alpha, rounds, q,
    clf_name, query_strategy,
    seed,
    init_per_client=2,
    enforce_max_tries=5
):
    rng_global = make_rng(seed)

    # 1) partition clients (indices into X_train/y_train)
    clients_indices = dirichlet_label_skew_partition(y_train, K=K, alpha=alpha, seed=seed)
    pool_dist_df = summarize_client_pools(y_train, clients_indices, seed, K, alpha)

    # 2) init labeled/unlabeled per client
    L_sets, U_sets = [], []
    round_state_rows = []

    for cid, idx in enumerate(clients_indices):
        perm = idx.copy()
        rng_global.shuffle(perm)

        L = perm[:min(init_per_client, len(perm))]
        U = perm[min(init_per_client, len(perm)):]

        L = np.asarray(L, dtype=int)
        U = np.asarray(U, dtype=int)

        L_sets.append(L)
        U_sets.append(U)

        # round 0 log: forced sampling not applied at init here
        round_state_rows.append(
            log_round_state(y_train, seed, K, alpha, round_id=0, client_id=cid, L_idx=L, U_idx=U, forced_random_sampling=0)
        )

    # 3) run rounds
    w_global = None
    b_global = 0.0

    for r in range(1, rounds + 1):
        local_params = []
        local_weights = []

        for cid in range(K):
            L_idx = L_sets[cid]
            U_idx = U_sets[cid]

            # Capture original L before any forced sampling
            original_L_idx = L_idx.copy()
            forced_sampling = not has_two_classes(y_train, original_L_idx)

            # Deterministic RNG for this (seed, round, client)
            rng_local = make_rng(seed * 10_000 + r * 100 + cid)

            # ---------- one-class safeguard ----------
            if forced_sampling:
                L_new, U_new = enforce_two_classes(
                    y_train, L_idx, U_idx,
                    rng=rng_local,
                    max_tries=enforce_max_tries
                )
                L_sets[cid] = L_new
                U_sets[cid] = U_new
                L_idx, U_idx = L_new, U_new

            # If still single-class, fall back to current global model (matches your original behavior)
            if not has_two_classes(y_train, L_idx):
                if w_global is not None:
                    local_params.append((w_global, b_global))
                    local_weights.append(len(L_idx))

                    # log state (forced_sampling remains whatever triggered above)
                    round_state_rows.append(
                        log_round_state(y_train, seed, K, alpha, round_id=r, client_id=cid, L_idx=L_idx, U_idx=U_idx, forced_random_sampling=forced_sampling)
                    )
                    continue
                else:
                    # first round edge case: no global model yet, skip client
                    round_state_rows.append(
                        log_round_state(y_train, seed, K, alpha, round_id=r, client_id=cid, L_idx=L_idx, U_idx=U_idx, forced_random_sampling=forced_sampling)
                    )
                    continue

            # Train local model
            need_proba = (query_strategy == "lc")  # margin/random do NOT need proba in your method

            clf = make_classifier(clf_name, seed + r * 100 + cid, need_proba=need_proba)

            # ---- guard: if LC needs proba but labeled set too small for calibration ----
            if need_proba and isinstance(clf, CalibratedClassifierCV):
                yL = y_train[L_idx]
                _, counts = np.unique(yL, return_counts=True)
                if np.min(counts) < CALIB_CV:
                    # not enough samples per class for cv calibration -> fallback to random query this round
                    # (and train an uncalibrated base model just to keep parameters defined)
                    base_name = clf_name  # same underlying model
                    clf = make_classifier(base_name, seed + r * 100 + cid, need_proba=False)

            clf.fit(X_train[L_idx], y_train[L_idx])



            # Query (normal AL) — only after we ensure two classes
            if len(U_idx) > 0 and q > 0:
                if query_strategy == "random":
                  chosen = query_by_random(U_idx, q, rng_local)

                elif query_strategy == "margin":
                  chosen = query_by_margin(clf, X_train, U_idx, q)

                elif query_strategy == "lc":
                  if hasattr(clf, "predict_proba"):
                    chosen = query_by_least_confidence(clf, X_train, U_idx, q)
                  else:
                    chosen = query_by_random(U_idx, q, rng_local)

                else:
                  raise ValueError(f"Unknown query_strategy: {query_strategy}")



                chosen = np.asarray(chosen, dtype=int)
                L_idx = np.asarray(L_idx, dtype=int)
                U_idx = np.asarray(U_idx, dtype=int)

                chosen_set = set(chosen.tolist())

                if chosen.size > 0:
                    new_L = np.concatenate([L_idx, chosen]).astype(int)
                else:
                    new_L = L_idx.astype(int)

                new_U = np.asarray([int(i) for i in U_idx if int(i) not in chosen_set], dtype=int)

                L_sets[cid] = new_L
                U_sets[cid] = new_U
                L_idx, U_idx = new_L, new_U


                # retrain after labeling
                clf.fit(X_train[L_idx], y_train[L_idx])

            # Extract linear params and FedAvg
            w, b = _extract_linear_params(clf)
            local_params.append((w, b))
            local_weights.append(len(L_idx))

            # per-round logs
            round_state_rows.append(
                log_round_state(y_train, seed, K, alpha, round_id=r, client_id=cid, L_idx=L_idx, U_idx=U_idx, forced_random_sampling=forced_sampling)
            )

        # FedAvg update (only if at least one client contributed)
        if len(local_params) > 0:
            w_global, b_global = fedavg_linear(local_params, local_weights)
    '''
    # 4) Evaluate global linear params on test
    y_pred = linear_predict_from_params(X_test, w_global, b_global)
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    p = precision_score(y_test, y_pred)
    r = recall_score(y_test, y_pred)
    '''
    # 4) Evaluate global linear params on test
    y_pred = linear_predict_from_params(X_test, w_global, b_global)
    acc  = accuracy_score(y_test, y_pred)
    p   = precision_score(y_test, y_pred, average="weighted", zero_division=0)
    r   = recall_score(y_test, y_pred, average="weighted", zero_division=0)
    f1  = f1_score(y_test, y_pred, average="weighted", zero_division=0)

    result_row = {
        "seed": seed,
        "K": K,
        "alpha": alpha,
        "rounds": rounds,
        "q": q,
        "init_per_client": init_per_client,
        "classifier": clf_name,
        "query": query_strategy,
        "accuracy": acc,
        "f1": f1,
        "p": p,
        "r": r
    }

    round_state_df = pd.DataFrame(round_state_rows)
    return result_row, pool_dist_df, round_state_df

"""## Run the ablation study"""

# ===== Cell 12: Experiment grid + strict held-out test split =====
# EDIT THESE
ALPHA = 0.7
TEST_SIZE = 0.3

SEEDS = list(range(42, 52))          # 42..51 (10 seeds)
K_LIST = [2, 3, 4, 5, 6, 7, 8, 9]              # edit to your sweep
Q_LIST = [1, 2, 3, 4, 5]                 # labels per client per round
ROUNDS_LIST = [5, 10, 15, 20]               # edit
CLASSIFIERS = ["linearsvc", "sgd_hinge", "logreg"]  # add "logreg" if you want
QUERIES = ["random", "lc", "margin"]

# Strict held-out test split per seed (as you did)
# We'll rerun split per seed to reflect that seed controls split.
print("Grid size:", len(SEEDS) * len(K_LIST) * len(Q_LIST) * len(ROUNDS_LIST) * len(CLASSIFIERS) * len(QUERIES))

!pip install -q sentence-transformers scikit-learn numpy pandas openpyxl

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import LinearSVC
from sklearn.calibration import CalibratedClassifierCV
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import shuffle
import random
import itertools

from sentence_transformers import SentenceTransformer

# ===== Cell 13: Run experiments (collect results + logs) =====
all_results = []
all_pool_dists = []
all_round_logs = []

for seed in SEEDS:
    X_tr, X_te, y_tr, y_te = train_test_split(
        X, y, test_size=TEST_SIZE, stratify=y, random_state=seed
    )

    for K in K_LIST:
        for q in Q_LIST:
            for rounds in ROUNDS_LIST:
                for clf_name in CLASSIFIERS:
                    for query in QUERIES:
                        print(f"")
                        print(f"Experiment no. {len(all_results) + 1}>> K={K}, q={q}, rounds={rounds}, clf={clf_name}, query={query}, seed={seed}")
                        row, pool_df, round_df = run_seqfal(
                            X_train=X_tr, y_train=y_tr,
                            X_test=X_te, y_test=y_te,
                            K=K, alpha=ALPHA, rounds=rounds, q=q,
                            clf_name=clf_name, query_strategy=query,
                            seed=seed,
                            init_per_client=2
                        )
                        all_results.append(row)
                        all_pool_dists.append(pool_df)
                        all_round_logs.append(round_df)

results_df = pd.DataFrame(all_results)
pool_dist_df = pd.concat(all_pool_dists, ignore_index=True) if all_pool_dists else pd.DataFrame()
round_log_df = pd.concat(all_round_logs, ignore_index=True) if all_round_logs else pd.DataFrame()

print("Done.")
print("results_df:", results_df.shape)
print("pool_dist_df:", pool_dist_df.shape)
print("round_log_df:", round_log_df.shape)

# Aggregate summary (mean±std over seeds) =====
group_cols = ["K", "alpha", "rounds", "q", "init_per_client", "classifier", "query"]
summary = (
    results_df
    .groupby(group_cols)
    .agg(
        acc_mean=("accuracy", "mean"),
        acc_std=("accuracy", "std"),
        f1_mean=("f1", "mean"),
        f1_std=("f1", "std"),
        p_mean=("p", "mean"),
        p_std=("p", "std"),
        r_mean=("r", "mean"),
        r_std=("r", "std"),
        n_runs=("accuracy", "count")
    )
    .reset_index()
)

summary.head()

results_df.to_csv("results.csv", index=True)
summary.to_csv("summary.csv", index=True)
pool_dist_df.to_csv("pool_dist.csv", index=True)
round_log_df.to_csv("round_log.csv", index=True)

